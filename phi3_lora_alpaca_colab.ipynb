{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2edb6d52",
      "metadata": {},
      "source": [
        "# Phi-3 Mini LoRA on Alpaca 52K\n",
        "\n",
        "This notebook runs the full pipeline: clone the repository, install dependencies, train the LoRA adapter, evaluate, and run interactive chat.\n",
        "\n",
        "**Before running:** Set the runtime to use a GPU (Runtime > Change runtime type > T4 GPU or higher)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/xuxinjo/phi3-lora-alpaca.git\n",
        "%cd phi3-lora-alpaca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q torch transformers peft datasets bitsandbytes accelerate sentencepiece evaluate tqdm\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Verify GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6710b6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPU detected. Training will be very slow. Enable a GPU in Runtime > Change runtime type.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training\n",
        "\n",
        "Fine-tune Phi-3 Mini with LoRA on the Alpaca 52K dataset. The adapter is saved to `checkpoints/lora_phi3`. This may take a few hours on a T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python main.py train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation\n",
        "\n",
        "Compare the base model and the fine-tuned model on perplexity, ROUGE-L, and BLEU. Requires that training has completed and `checkpoints/lora_phi3` exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python main.py eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Interactive chat\n",
        "\n",
        "Run the chat interface. When the cell runs, type your instruction in the input box and press Enter. Type `quit` or `q` to exit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python main.py chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Optional: Merge LoRA into the base model\n",
        "\n",
        "Merge the trained LoRA adapter into the base Phi-3 model and save a single standalone checkpoint to `merged_model/phi3_alpaca_lora_merged`. Use this if you want to export a full model without loading the adapter separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python src/models/merge_lora.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
